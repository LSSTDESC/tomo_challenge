\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[twocolumn,twocolappendix]{aastex63}

% Add your own macros here:
\pdfoutput=1 %for arXiv submission
%\usepackage{amsmath,amssymb,amstext}
\usepackage{amsmath,amstext}
\usepackage[T1]{fontenc}
\usepackage{apjfonts}
\usepackage{ae,aecompl}
\usepackage[utf8]{inputenc}
\usepackage[figure,figure*]{hypcap}
\usepackage{natbib}
\usepackage{url}
\usepackage{listings}

\urlstyle{same}

\usepackage{lineno}
\linenumbers
%\modulolinenumbers[2]

\newcommand{\placeholder}[1]{\textit{PLACEHOLDER: #1}}


% header settings
\shorttitle{Tomographic binning optimization}
\shortauthors{Author A et al.\ (LSST~DESC)}

% ======================================================================

\begin{document}
\title{The LSST DESC 3x2pt Tomography Optimization Challenge}
\input{authors}

\begin{abstract}
In this paper we present the results of the DESC 3x2pt tomography challenge, which aims to compare strategies for optimizing the photometric binning required for the main LSST 3x2pt analysis. This task is made particularly delicate in the context of a metacalibrated lensing survey, as only the photometry from the bands  included in the metacalibration process (riz and potentially g) can be used to define this tomographic binning. Using a set of realistic simulations including true galaxy redshifts and simulated photometry, the goal of the challenge is to propose a bin assignment strategy that maximizes the overall Figure of Merit of a 3x2pt analysis. This setting is idealized as it ignores spectroscopic completeness  issues for the training sample, but complex enough to address the main question of binning optimization. Based on the challenge dataset, we review and compare the different methods that have been proposed by participants. We find that even from this limited photometry information, various methods are able to yield binning schemes that achieve high FoM scores.
\end{abstract}

\keywords{methods: statistical -- dark energy  -- large-scale structure of the universe}

%\accepted{}
%\submitjournal{the Astrophysical Journal Supplement}


%\tableofcontents%-----------------------------
%===========================
% BEGINNING OF THE MAIN TEXT
%===========================

\section{Introduction}
Weak gravitational lensing (WL) has emerged over the last decade as a powerful
cosmological probe \citep{cfhtlens,des,kids,hsc}.  WL
uses measurements of coherent shear distortion to the observed shapes of galaxies
to track the evolution of large-scale gravitational fields.  It measures the integrated
gravitational potential along lines of sight to source galaxies, and can thence constrain
the laws of gravity, the expansion history of the Universe, and the history and growth
of cosmic structure.

WL has proven especially powerful in combination with galaxy clustering measurements,
which can measure the density of matter up to an unknown bias function.  The high signal
to noise of such measurements and the relative certainty of the redshift of these foreground
samples breaks degeneracies in the systematic errors that affect WL.

The \emph{3x2pt} method has become the standard method for performing this combination.
In this method, two-point correlations are computed among and between two samples, the shapes of 
background (source) galaxies and the locations of foreground galaxies, which trace foreground
dark matter haloes.  The three combinations (source-source, source-lens, and lens-lens) are
measured in either Fourier or configuration space, and can be predicted from a combination of 
perturbation theory and simulation results.  The method has been used in the Dark Energy Survey, DES
\citep{des-3x2pt}, and to combine Baryon Oscillation Spectroscopic Survey, BOSS, with the Kilo-Degree 
Survey, KiDS \citep{kids-3x2pt}.

Most lensing and 3x2pt analyses have been performed \emph{tomographically}, 
binning galaxies by redshift.
This approach captures almost all the available information in lensing data, since it measures
an integrated effect and so galaxies nearby in redshift probe very similar fields.  For photometric
foreground samples it is similarly near-lossless, since redshift estimates of such galaxies have
large uncertainties\footnote{Spectroscopic foreground samples may be more likely to see significant 
gains from moving beyond spectroscopic methods.}.  Binning galaxies by approximate redshift lets us 
model galaxy bias, intrinsic alignments, and other systematic errors en masse in a more tractable way.
While fully 3D methods have been proposed and prototyped \citep{kitching,more}, the tomographic 
approach remains the standard within the field.

Tomographic 3x2pt measurements will be a key science goal in the upcoming \emph{Stage IV} surveys,
including the Rubin Observatory \citep{rubin} and the Euclid and Roman space telescopes \citep{euclid,roman}.

In this paper we discuss and evaluate methods for assigning objects to tomographic bins, and 
specifically the impact
of using a limited set of color bands to make those assignments, motivated by requirements of using the
metacalibration method for galaxy shape measurement to limit biases.  Tomographic methods have been 
explored before in \citet{jain} and \citet{kitching2019}.  In this paper we describe the results of a 
challenge using simulated Rubin-like data with a range of methods submitted by different teams.  We explore how many tomographic bins we can effectively generate using only a subset of bins, and compare methods submitted to the challenge.

In \autoref{sec:motivation} we explain the need for methods that work on limited sets of bands in the context of upcoming lensing surveys.  \textbf{Complete structure...}

\section{Motivation}
\label{sec:motivation}

The methodology used to assign galaxies to bins faces special challenges when we use a particularly
effective approach for measuring galaxy shear, called \emph{metacalibration} (metacal),
which was introduced in \citet{sheldonhuff} and developed further in \citet{sheldon}.
In metacal, galaxy images are deconvolved from the point-spread function (PSF), sheared, and
reconvolved before measurement, allowing one to directly compute the response of a metric
to underlying shear and correct for it when computing two-point measurements.  This can
almost completely remove errors due to model and estimator (noise) bias from WL data, at least
when PSFs are well-measured and oversampled by the pixel grid.

Furthermore, metacal provides a solution to the pernicious and general problem of
\emph{selection biases} in WL.  Measurements of galaxy shear are known to have noise-induced errors
that are
highly covariant with those on other galaxy properties, including size and, most importantly, flux/
magnitude.  It follows that a cut (or re-weighting) on galaxy magnitudes, or on any quantity derived
from  them, will induce a shear bias since galaxies will preferentially fall either side of the cut
depending on their shear.  Within metacal, we can solve this problem by performing the cut on both
sheared variants of the catalog, and measuring the difference between the post-cut shear in the two
variants.  In DES the corrected biases were found up to around 3\%, far larger than
the requirements for the current generation of surveys \cite{des-y1-cat}.

In summary: metacal allows us to correct for significant selection biases, but only if all selections
are performed using only bands in which the PSF is measured well enough for deconvolution to be
performed.  For Rubin, this means using only the $r$, $i$, $z$, and perhaps $g$ bands.  In this work we
therefore study how well we can perform tomography using only this limited photometric information.  In
particular, this limitation prevent us from using the most obvious method for tomography,
and computnig a complete photometric probability density function (PDF) for each galaxy and assigning
a bin using the mean or peak of that PDF.

Simulation methods can also be used to correct for selection biases, provided that they match the
real data to high accuracy.  If we determine that limiting the bands we can use for tomography results
in a significant decrease in our overall signal-to-noise, outweighing the gain from the improved shape
calibration then this might suggest moving to rely more on such methods\footnote{One can account for
residual shear estimation uncertainty during parameter estimation by marginalizing over an unknown
factor.  Widening the prior on this factor decreases the overall constraining power of the analysis.}.


\section{Challenge Design}

The Dark Energy Science Collaboration (DESC) Tomographic Challenge was launched in May 2019 and 
advertised among the LSST cosmology community, though it was open to outside entrants.

In the challenge, participants assigned simulated galaxies to tomographic bins,
using only their (g)riz bands and associated errors.  Their goal was to maximize the cosmological
information in the final split data set; this is generally acheived by separating different bins by 
redshift as much as possible.  Particpants were free to optimize or simply select nominal (target) 
redshift bin edges from their training sample, but in either case had to classify galaxies into the 
different bins.

\subsection{Philosophy}

Since this was a preliminary challenge designed to explore whether it is possible \emph{in theory}
to use only (g)riz for tomography, we chose to simplify several aspects of the data.  In realistic 
situations we will have access to limited sized training sets for WL photometric redshifts, since 
spectra are time-consuming to obtain.  These training sets will also be highly incomplete compared
to full catalogs, especially at faint magnitudes where spectroscopic coverage is sparse.  In this
challenge we avoided both these issues -- the training samples we provided (see below) were comparable
in size to the testing sample, and drawn from the same population.  Given this, the challenge
represents a best-case scenario -- if no method succeeded on this easier case then more realistic
cases would probably be impossible.

Despite these simplifications, the other aspects of the challenge and process were designed to be
as directly relevant as possible to the particular WL science case we focus on.  The data set
was chosen to mimic the population, noise, and cuts we will use in real data (see \autoref{sec:data}, 
and the metrics were designed to be as close to the science goals as possible, rather than a lower
level representation (see \autoref{sec:metrics}). The data set was also large enough to pose a 
reasonably realistic test of methods at the large data volume required for upcoming surveys, where
$10^9$ -- $10^{10}$ galaxies will be measured.

The challenge was open to any entrants, not just those already involved in DESC, but it was
advertised primarily through Rubin, and so is perhaps best described as semi-public.
Participants tested and ran their methods locally, but submitted code to be run as part of a central
combined analysis.  No prize was offered beyond recognition.


\subsection{Data}
\label{sec:data}


\begin{itemize}
    \item CosmoDC2 - description
    \item CosmoDC2 - discussion
    \item Noise properties
    \item Pre-selection 
    \item Metacal as a limitation
\end{itemize}

\subsection{Metrics}
\label{sec:metrics}

In this challenge we use only a lensing source (background) population, since the metacal requirements
do not apply to the foreground lens sample. We do, however, calculate 3x2pt metrics using the
same source and lens populations, both because this scenario is one that will be used in some analyses,
and because clustering-like statistics of lensing samples are important for studies of intrinsic
alignments. We use two different sets of metrics, one based on the over signal to noise of the spectra 
derived from the samples, and one based on a figure-of-merit for the constraints that would be obtained 
using them.  \textbf{Discuss how correlated these turn out to be here}.  

For each bin $i$ generated by a challenge entry, we make a histogram of the true redshifts 
of each galaxy assigned to the bin.  This is then used as our number density $n_i(z)$ in the metrics 
below. Both metrics reward bins that can
cleanly separate galaxies by redshift, since this reduces the covariance between the samples. No
method can perfectly separate them, so good methods reduce the tails of $n_i(z)$ that overlap with 
neighbouring bins.

We developed two implementations of each of these metrics, one using the Core Cosmology Library 
\citep{ccl} and one the JAX Cosmology Library \citep{jax-cosmo}, which provides differentiable theory 
predictions using the JAX library \citep{jax}.  In production we use the latter since it provided a
more stable calculation of metric 2.

Metric 1 would be approximately measurable on real data without performing a full cosmology analysis,
whereas metric 2 would be the final output of such an analysis.  We therefore simulate performance
tests at multiple stages in the wider process.


\subsubsection{Metric 1: Sigal-to-noise}
Our first metric is the total signal-to-noise ratio of the power spectrum derived from the assigned
$n_i(z)$ values:
\begin{equation}
    S = \mu^{T} C^{-1} \mu
\end{equation}
where $\mu$ is a stack of the theoretical predictions for the $C_\ell$ spectra for each tomographic 
bin, in turn, and $C$ a Gaussian estimaate of the covariance between them, as described in
\citet{wl-review} and summarized in \autoref{app:theory}.   We compute this metric for lensing alone, clustering alone, and the full 3x2pt combination.

\subsubsection{Metric 2: Figure-of-merit}

Metric 2 uses a figure of merit based on that presented by the Dark Energy Task Force \citep{detf}.  We 
use the inverse area of a Fisher Matrix ellipse, which approximates the size in two axes 
of the posterior PDF one would obtain on analysing the spectra:
\begin{align}
    F &= \left( \frac{\partial \mu}{\partial \theta} \right)^T C^{-1} \left( \frac{\partial \mu}{\partial \theta} \right), \\
    S &= \frac{1}{2 \pi \sqrt{\det{([F^{-1}]_{p_1, p_2})}}}
\end{align}
where $[F^{-1}]_{p_1, p_2}$ extracts the 2x2 matrix for two parameters.  We use both the original
DETF variant in which $p_1 = w_0, p_2 = w_a$, representing the constraining power on dark energy 
evolution, and a version in which $p_1 = \Omega_c$ and $p_2 = \sigma_8$, representing constraining
power on overall cosmic structure amplitude and its evolution.




\subsection{Infrastructure}
\begin{itemize}
    \item Pull request submissions
    \item Data at NERSC
\end{itemize}


\subsection{Control Methods}

Two control methods were used to test the challenge machinery and metrics.
The first method randomly assigned galaxies to each tomographic bin, resulting
in bins which each had the same $n(z)$, on average.  Using this method, we expect
that increasing the number of bins should not increase any metric, since bins are
maximally correlated.  We find this to be true for all our metrics.

Second, we use a method which uses only the $i$-band to select tomographic bins,
splitting galaxies in equal-sized bins in this magnitude. We expect only a small
increase in metrics with the number of bins, and also that the addition of the $g$-band
should make no difference to metrics.  Again, we find this to be true.

One additional method was submitted to the challenge with the caveat that it could
never be applied to real data and should therefore considered a form of control method.
This method was {\sc Utopia}, which found, for each object in the testing data, the
nearest neighbour object in the training sample.  The high density, completeness, and
representativeness of our training set made this possible; realistics spectroscopic training
sets are too sparse.  The method performed well, as expected, 
but was not the highest scoring overall.


\section{Methods}

\subsection{ {\sc RandomForest} }
This method was submitted by the challenge organizers and used a random
forest algorithm to assign galaxies.

Random forests \cite{breiman2001} train a collection of individual \emph{decision trees},
each of which consists of a set of bifurcating comparisons in which different features in
the data (in this case, magnitudes and errors) are compared to criterion values to choose
which branch to follow.  Each ``leaf'' (end comparison) selects a classification bin for
an input galaxy, and the tree is trained to choose comparison features and comparison values
which maximally split the discrimination at each step, and in final leaf purity.

Since decision trees tend to over-fit data, random forests generate a suite of decision
trees, each randomly choosing a subset of features on which to train at each bifurcation.
An average over the trained trees is taken as the overall classification.

In this implementation we chose nominal bin edges by splittng ensuring equal numbers of training set 
objects were in each bin, and then trained the forest to map magnitudes, magnitude errors, and
galaxy sizes to bin indices.  We use the {\sc scikit-learn} implementation of the random forest
\citep{scikit-learn}.


% Please describe your methods with some very general / high level background theory
% and a focus on the spsecific implementation.  Add any citations you like with bibtex in
% paper.bib
% Feel free to rename your method, but please leave a comment explaining so that I can do the
% same in the tables.

\subsection{ {\sc LSTM} } 
\label{ClecioLSTM} 
Methods \ref{ClecioLSTM} - \ref{ClecioEnsemble} were
submitted by a team of authors CRB, BMOF, GT, ESC, and EJG.  All were built using TensorFlow, and
chose bin edges such that an equal number of galaxies is assigned to each bin, and trained the
network using the galaxies' magnitudes and colours. 

The LSTM method uses a Bidirectional Long Short Term Memory network to assign galaxies to redshift
bins.
 
Recurrent Neural Networks (RNNs)\citep{schuster, medsker, pascanu} are a type of NN capable of
analyzing sequential data, where data points have a strong correlation with their neighbours. Long
Short Term Memory (LSTM) units are a particular type of RNN capable of retaining relevant
information from previous points while at the same time removing unnecessary data. This is achieved
through a series of gates with learnable weights connected to different activation functions to
balance the amount of data retained or removed.
 
In some cases, relevant information can come both from data points coming before or after each point
being analyzed. In such cases, two LSTMs can be combined, each going in a different direction, to
create effectively a bidirectional LSTM cell \citep{schuster}.
 
In this implementation, we chose bin edges such that an equal number of galaxies is assigned to each
bin, and trained the network using the galaxies' magnitudes and colours. The net was implemented
using Tensorflow and Keras. The Deep model is composed of a series of convolutional blocks (1d
convolutional layer with a tanh activation followed by a MaxPooling layer) followed by a
bidirectional LSTM layer and a series of fully connected layers. 

\subsection{ {\sc Autokeras LSTM}}
Autokeras \citep{autokeras} is an AutoML system based on Keras. Given a general neural network
architecture and a dataset, it searches for the best possible detailed configuration for the problem
at hand. 
 
We started from the basic architecture and bin assignment scheme mentioned in Section
\ref{ClecioLSTM} and let Autokeras search for the configuration that results in the lowest
validation loss. We kept the order of the layer blocks fixed, i.e., convolutional blocks,
bidirectional LSTM, and Dense blocks. We let the other hyperparameters free, such as the number of
neurons,  Dropout rates, maxpooling layers, the number of convolutional filters, and strides.


\subsection{ {\sc LGBM} }
This method uses a Light Gradient Boosted Machine (LightGBM) \citep{lgbm} to assign galaxies to
redshift bins.
 
LightGBM uses a histogram-based algorithm that assigns continuous feature values to discrete bins
instead of other pre-sort-based algorithms for decision tree learning. LightGBM also grows the trees
leaf-wise, which tends to achieve lower losses than level-wise tree growth. This method tends to
overfit for small data so that a \textit{max\_depth} parameter can be specified to limit tree depth.

 
In this implementation, we chose bin edges such that an equal number of galaxies is assigned to each
bin, and trained the network using the galaxies' magnitudes and colours.

 
\subsection{ {\sc TCN}}
This method uses a Temporal-Convolutional (TCN) \citep{baitcn} network to assign galaxies to redshift
bins.
 
Recurrent Neural Networks (such as LSTMs) are considered to be the state-of-the-art model for
sequence modelling. However, research indicates that certain convolutional architectures can achieve
human-level performance in these tasks \citep{dauphin}.
 
TCNs are Fully Convolutional Networks with causal convolutions, in which the output at time
\textit{t} is convolved only with elements of time \textit{t} and lower in the previous layer. This
new approach prevents leakage of information from the future to the past. This simple model has one
disadvantage, in that it needs a very deep architecture or large filters to achieve a long history
size. 
 
By using dilated convolutions, where a fixed step is introduced between adjacent filter taps, the
receptive field of the TCN can be increased without increasing the number of convolutional filters.
Furthermore, residual blocks \citep{he} are also used to stabilize deeper and larger networks. These
modifications cause TCNs to have a longer memory than traditional RNNs with the same capacity,  and
also require low memory for training.
 
We used the Keras implementation of TCN \citep{kerastcn} and chose bin edges such that an equal
number of galaxies is assigned to each bin, and trained the network using the galaxies' magnitudes
and colours. 

\subsection{ {\sc CNN} } 
This method uses an optimized Convolutional Neural Network (CNN) to assign
galaxies to redshift bins. 
 
CNNs \citep{lecun2015deep} are layers inspired in pattern recognition types developed in mammals' brains.
They consist of kernels that are convolved with the data as it flows through the net. CNN-based
models have emerged as state-of-the-art in several computer vision tasks, such as image
classification and pose estimations, as well as having applicability in a range of different fields.
 
We combine the Convolutional layers are with dimensionality reduction (pooling layers) and different
activation functions. Here, we used Autokeras to optimize a general convolutional architecture,
using the galaxies' magnitudes and colours to assign them to redshift bins, which were chosen such
that an equal number of galaxies were assigned to each.


\subsection{ {\sc Ensemble} } 
\label{ClecioEnsemble} 
This method combines different neural network
architectures to assign galaxies to redshift bins.
 
Deep Ensemble models combine different neural network architectures to obtain better performance
than any of the nets alone. In this method, we used the Bidirectional LSTM optimized by Autokeras, a
ResNet \citep{resnet} and a Fully Convolutional Network (FCN) \citep{fcn}. The predictions of these
models were averaged to get the final predicted bin. 
 


\subsection{ {\sc ComplexSOM} }


\subsection{ {\sc ENSEMBLE} }


\subsection{ {\sc GPzBinning} }
This method was submitted by author PH.

GPzBin is based on GPz \citep{Almosallam2016a,Almosallam2016b}, a machine learning
regression algorithm originally developed for calculating the photometric
redshifts of galaxies \citep{Gomes2017,Duncan2018,Hatfield2020} (but now also
applied to other problems in physics \citep{Peng2019,Hatfield2020}). The
algorithm is based on a Gaussian Process (GP; essentially an un-parametrised continuous
function defined everywhere with Gaussian uncertainties); GPz specifically uses 
a sparse GP framework, a fast and a scalable approximation
of a full process \citep{Rasmussen2006}.

The mean $\mu(x)$ and variance $\sigma(x)^2$ for predictions as a function of
the input parameters $x$ (typically the photometry) are both linear combinations
of a finite number of `basis functions': multi-variate Gaussian kernels with
associated weights, locations and covariances. The algorithm seeks to find the
optimal parameters of the basis functions such that the mean and variance
functions are the most likely functions to have generated the data. The key
innovations introduced by GPz include a) input-dependent variance estimations
(heteroscedastic noise), b) a `cost-sensitive learning' framework where the
algorithm can be tailored for the precise science goal, and c) properly
accounting for uncertainty contributions from both variance in the data
($\beta^{-1}_{\star}$) as well as uncertainty from lack of data ($\nu$) in a
given part of parameter space (by marginalising over the functions supported by
the GP that could have produced the data).

%During the training stage of the algorithm GPz is inferring the locations and
%spreads that describe the basis functions; during the validation stage it is
%inferring the appropriate complexity of the model, essentially how many basis
%functions to use.


GPzBin is a simple extension to GPz for the problem of tomographic binning. For
a given number of tomographic bins it selects redshift bin edges $z_{i}$ such
that there are an equal number of training galaxies in each bin. Testing data
galaxies then are assigned to the bin corresponding to their GPz predicted
photometric redshift. The code allows for two selection criteria for removing
galaxies for which it was not possible to make a good photometric redshift
prediction: 1) cutting galaxies close to the boundaries of bins based on an
`edge strictness' parameter $U$ which removes galaxies where $\mu\pm U \times
\sigma \lessgtr z_i$ and 2) cutting galaxies based on the degree $E$ to which
GPz is having to extrapolate to make the redshift prediction, removing galaxies
with $\nu>E\sigma^2$ (see Figure 12 of \cite{Hatfield2020}). Cutting galaxies
removes some signal at the possible benefit of improving binning purity. As the
training and test data were sampled from the same distribution in this data
challenge we might not expect removing galaxies to give huge improvements in
binning quality, but cuts based on $E$ and $U$ might become more useful in
future studies where the training and test data have substantially different
colour-magnitude distributions.

When used in GPzBin for this data challenge the GPz settings
in Table \ref{table-settings} were used (see
\cite{Almosallam2016a,Almosallam2016b} for precise definitions and
interpretations).

%GPz requires very little fine-tuning. The most important parameter is $m$, the number of basis functions. A higher $m$ corresponds to higher model complexity and longer training times.

\begin{table*}[]
% \tiny
\begin{center}
    \begin{tabular}{| l | l | l |}
    \hline
    Parameter   &   Value   &   Description\\  
    \hline 
    $m$         &   100     &   Number of basis functions; complexity of GP\\

    maxIter     &   500     &   Maximum number of iterations (comparisons with 
                                the validation data) permitted\\

    maxAttempts &   50      &   Maximum number of iterations to attempt if there 
                                is no progress on the validation set\\

    method      &   GPVC    &   Type of bespoke covariances used on each basis 
                                function (see \cite{Almosallam2016b} for 
                                the different options)\\

    normalize   &   True    &   Pre-process the input by subtracting the means
                                and dividing by the standard deviations\\

    joint       &   True    &   Jointly learn a prior linear mean-function 
                                (learn the function means and variances 
                                jointly)\\

    heteroscedastic  & True & Model noise as well as point estimates \\
    \hline
    \end{tabular}
\caption{Parameter settings in the GPz entry.}
\end{center}
\label{table-settings}
\end{table*}

\subsection{ {\sc JaxCNN} }


\subsection{ {\sc JaxResNet} }


\subsection{ {\sc NeuralNetwork 1} }


\subsection{ {\sc NeuralNetwork 2} }


\subsection{ {\sc PCACluster} }
This entry was submitted by author DG.

Principal Component Analysis (PCA) reduces the dimensionality of a dataset by
calculating eigenvectors that align with the principal axes of variation in the
data \citep{doi:10.1098}. {\sc PCACluster} uses PCA to reduce the flux \& color
data set to three dimensions. 

To generate the PCA dimensions used by this method the PCA algorithm takes in
the $r$-band flux and $ri$ and $iz$ colors and outputs three principal
components.  If the data set provides the $g$-band, the algorithm also uses the
$gr$ color when determining eigenvectors. Tomographic bins are then determined
in this three dimensional space using a unique clustering algorithm where each
observation is assigned to a bin by determining which cluster centroid is
closest to that observation.

In order to determine optimal centroid positions, clustering is framed as a
gradient descent problem that seeks to maximize the requested metric: either the
SNR or the FoM. This approach of identifying clusters using gradient descent is
inspired by the similar approach to K-means clustering in
\cite{NIPS1994_a1140a3d}.

This entry uses {\sc jax\_cosmo} to take the derivative of the requested metric
and classification function combination with respect to the centroid locations
to calculate gradients and weight updates \citep{jax-cosmo}.  The speed of
training PCACluster is directly proportional to the number of requested
centroids (the number of requested tomographic bins) and the amount of training
data used.


\subsection{ {\sc SimpleSOM} }


\subsection{ {\sc UTOPIA} }


\subsection{ {\sc ZotBin} }


\subsection{ {\sc ZotNet} }


\subsection{ {\sc funbins} }


\subsection{ {\sc mlpqna} }
This entry was submitted by authors SC and MB.

The Multi Layer Perceptron trained by Quasi Newton Algorithm 
({\sc MLPQNA}; \citealp{Brescia12}), is a feed-forward neural network 
for multi-class classification and single/multi regression use cases.

The architecture is a classical Multi Layer Perceptron (MLP; \citealp{Rosenblatt1961})
with one or more hidden layers, on which the supervised learning paradigm is
based on the Quasi Newton Algorithm (QNA), implemented through the L-BFGS rule
\citep{Nocedal80}. L-BFGS is a solver that approximates the Hessian matrix,
representing the second-order partial derivative of a function. Further, it
approximates the inverse of the Hessian matrix to perform parameter updates.
MLPQNA uses the least square loss function with the Tikhonov regularization
\citep{Tikhonov77} for regression. It uses the cross-entropy \citep{deBoer05}
and softmax \citep{Sutton98} as output error evaluation criteria for
classification.

Besides several scientific contexts and projects in which MLPQNA has succesfully
been used, it was recently the kernel embedded into the method {\sc
METAPHOR} \citep{cavuoti20} for photo-z estimation, which participated in the
LSST Photometric Redshift PDF Challenge \citep{schmidt20}. 

The {\sc MLPQNA} python implementation used here
uses a customized Scipy version of the L-BFGS built-in rule.
It used two hidden layers with $2N-1$ and $N+1$ nodes respectively,
where $N$ is the number of input features (depending on the experiment).
The decay was set to 0.1.


\subsection{ {\sc Stacked Generalization} }
This method was submitted by author JEC.

\emph{Stacked generalization} consists of stacking the output of several individual estimators and 
using a classifier to compute the final prediction. Stacking methods use the strength of each 
individual estimator by using their outputs as input of a final estimator.  This entry stacked
a \emph{Gradient Tree Boosting} (GTB) classifier and then used a Logistic Regression as the final step.

GTB is a machine learning method which combines decision trees (like the Random Forest, RF)
by performing a weighted mean average of individual trees \citep{Friedman:2002we,RefWorks:1634}. 
The prediction $F(X)$ of the ensemble of trees for a new sample $X$ is given by
\begin{equation}
F(X) = \sum_k w_k F_k(X)
\end{equation}
where $F_k(X)$ is the individual tree answer and $w_k$ a weight per tree. Unlike the Random Forest, the 
tree depth is rather small and the training  of the $k$-th tree is based on the errors of the
$(k-1)$-th tree.  Like the RF, a randomly chosen fraction of features ($\approx 50\%$) is 
omitted at each split, and this step is changed at each tree (leading to a Stochastic Gradient Descent 
algorithm).  GTB learning is essentially non-parallelizable and so cannot use a map-reduce paradigm, 
unlike RF.It suffers from the opposite problem to RF's overfitting, and is subject
to high bias, so one generally combines many weak individual learners.

The two methods have their own systematics; they notably differ in the feature importances. Stacking 
the two is therefore especially effective.  The submitted method used {\sc SKLearn}, and used 50 models 
of each type combined using its {\sc StackingClassifier}.

An iterative precedure was used to progressively change the target bin edges to optimize one of the metrics (FOMs or SNRs) between the trainings of the classifiers. In general, we recover different choices for each statistic, but the differences are relatively marginal. This procedure was done by hand for this challenge, but could be automated and applied to any kind of classifier.


% \lstset{language=Python}
% \lstset{frame=lines}
% \lstset{caption={The code}}
% \lstset{label={lst:code_direct}}
% \lstset{basicstyle=\footnotesize}
% \begin{lstlisting}
% estimators = [ 
% ('gd', make_pipeline(StandardScaler(),
%     GradientBoostingClassifier(n_estimators=n_estimators, verbose=1))),
 
% ('rf', make_pipeline(StandardScaler(),
%     RandomForestClassifier(n_estimators=n_estimators,verbose=1)))  ]
        
% classifier = StackingClassifier(estimators=estimators,
%                                final_estimator=LogisticRegression(max_iter=5000))
% \end{lstlisting}




\section{Results}

\begin{table*}[]
\begin{tabular}{|l|llll|llll|}
                & \multicolumn{4}{c|}{\textbf{riz}}      & \multicolumn{4}{c|}{\textbf{griz}}                                \\ \hline
\textbf{Method} & \textbf{3-bin} & \textbf{5-bin} & \textbf{7-bin} & \textbf{9-bin} & \textbf{3-bin} & \textbf{5-bin} & \textbf{7-bin} & \textbf{9-bin} \\ \hline
{\sc Autokeras\_LSTM } & 31.1 & 74.9    & 103.4    & nan    & 44.1             & 67.5             & 122.2             & 98.4\\
{\sc CNN } & 27.1 & 50.8    & 76.7    & 103.3    & 30.5             & 57.7             & 95.4             & 122.4\\
{\sc ComplexSOM } & 38.0 & 52.1    & 94.4    & 101.6    & 34.9             & 45.0             & 91.6             & 100.3\\
{\sc ENSEMBLE1 } & 65.6 & 65.6    & 94.8    & 0.0    & 0.0             & 0.0             & 0.0             & 0.0\\
{\sc funbins } & 36.8 & 82.5    & 122.1    & 141.8    & 42.1             & 100.4             & 142.5             & 167.2\\
{\sc GPzBinning } & 26.2 & 49.7    & 80.8    & 111.7    & 27.8             & 55.3             & 87.2             & 126.9\\
{\sc IBandOnly } & 38.0 & 50.0    & 54.4    & 57.3    & 38.0             & 50.0             & 54.4             & 57.3\\
{\sc JaxCNN } & 59.9 & 101.9    & 105.2    & nan    & 79.7             & 125.2             & 150.0             & nan\\
{\sc JaxResNet } & 73.7 & 111.1    & 131.6    & nan    & 82.2             & 126.0             & nan             & 161.5\\
{\sc LGBM } & 27.0 & 50.8    & 78.8    & 108.2    & 30.3             & 57.4             & 92.9             & 125.2\\
{\sc LSTM } & 26.7 & 51.1    & 81.4    & 103.6    & 30.2             & 57.1             & 95.9             & 126.9\\
{\sc mlpqna } & 39.3 & 64.7    & 93.2    & 121.8    & 42.8             & 72.2             & 109.0             & 133.7\\
{\sc myCombinedClassifiers } & 35.3 & 60.5    & 83.5    & 120.4    & 39.4             & 63.9             & 93.1             & 148.9\\
{\sc NeuralNetwork1 } & 76.3 & 117.6    & 135.8    & 0.0    & 81.9             & 132.9             & 158.0             & 0.0\\
{\sc NeuralNetwork2 } & 30.5 & 48.0    & 57.8    & 136.1    & 49.4             & 102.7             & 122.2             & 140.0\\
{\sc PCACluster } & 29.5 & 68.1    & 75.8    & 0.0    & 50.1             & 72.7             & 90.8             & 0.0\\
{\sc Random } & 1.2 & 1.2    & 1.2    & 1.2    & 1.2             & 1.2             & 1.2             & 1.2\\
{\sc RandomForest } & 39.5 & 65.2    & 93.2    & 118.5    & 43.0             & 72.8             & 110.1             & 136.5\\
{\sc SimpleSOM } & 39.5 & 59.9    & 77.9    & 98.5    & 42.2             & 73.2             & 108.0             & 134.8\\
{\sc TensorFlow\_FFNN } & 39.0 & 64.3    & 90.3    & 114.7    & 43.0             & 72.0             & 112.3             & 137.2\\
{\sc TCN } & 40.0 & 65.3    & 90.9    & 110.5    & 44.1             & 72.8             & 108.0             & 134.0\\
{\sc UTOPIA } & 39.0 & 63.7    & 90.8    & 112.2    & 43.1             & 74.0             & 112.8             & 139.3\\
{\sc ZotBin } & 64.8 & 106.4    & 121.8    & 135.3    & 77.6             & 120.0             & 141.8             & 154.7\\
{\sc ZotNet } & 73.6 & 111.2    & 131.8    & 145.9    & 83.7             & 128.5             & 150.1             & 167.2\\
\end{tabular}
\caption{INCOMPLETE, AND AUTO-GENERATED SO NO POINT EDITING! Values of the 3x2pt DETF figure-of-merit achieved by entrant methods on the 
CosmoDC2 part of the challenge.}
\end{table*}

\begin{itemize}
    \item Big table of results
    \item fisher matrix ellipses overplotted for top N methods
    \item Plots of n(z)
\end{itemize}

\section{Discussion}
\begin{itemize}
    \item Overall effectiveness of method classes
    \item Individual winner
\end{itemize}



\subsection{Individual winners}

RUN WINNERS WITH UGRIZY TO GET AN IDEA OF THE LOSS FROM DROPPING U AND Y.

\subsection{Large bin numbers}

It is clear from values above that fears about restricted photometry catastrophically limiting
the information needed for tomography are unfounded.  In both the CosmoDC2 and Buzzard analyses,
even the $riz$ bands alone are sufficient to split objects into nine reasonably separated
tomographic bins.
Once bins become this narrow the primary analysis concern will become the calibration of overall
bin $n(z)$ values, rather than initial tomography, so we consider this to be reassuring.

The inclusion of the $g$ band typically increases metrics by around 10--20\%, including for the
highest scoring methods.  This value should be weighed against the challenge of high-accuracy
PSF measurement for this band, but until the end of the survey this is unlikely to be
the limiting factor in overall precision.

\subsection{Future directions}

\begin{itemize}
    \item spectroscopic incompleteness
\end{itemize}

\section{Conclusions}

\bibliography{paper}


\appendix

\section{Theory}\label{app:theory}
In our metrics, the power spectrum between two tomographic bins $i$ and $j$ is computed using either the Core Cosmology Library \citep{ccl} or the JAX Cosmology Library \citep{jax-cosmo} as:
\begin{equation}
    C^{ij}_\ell = \int_0^{\infty} \frac{q_i(\chi) q_j(\chi)}{\chi^2} P\left(\frac{\ell +\frac{1}{2}}{\chi}, \chi \right) \mathrm{d}\chi
\end{equation}
where $\chi = \chi(z)$ is the comoving distance and $P$ the non-linear matter power spectrum at
a fiducial cosmology.  The kernel functions $q_i(\chi)$ are different for the lensing and clustering samples:
\begin{align}
    q^{\mathrm{cluster}}_i(\chi) &= n_i(\chi)\\
    q^{\mathrm{lensing}}_i(\chi) &= \frac{3}{2}\Omega_m \left(\frac{\mathrm{H}_0}{c}\right)^2 \frac{\chi}{a(\chi)} \int_\chi^{\infty} \frac{\chi' - \chi}{\chi'} n_i(\chi')\,\,\mathrm{d}\chi'
\end{align}
where $n_i(\chi) = n_i(z) \frac{\mathrm{d}z}{\mathrm{d}\chi}$.

We evaluate these at 100 $\ell$ values from $\ell_\mathrm{min}=100$ to $\ell_\mathrm{max}=2000$.

We use a Gaussian estimate for the covariance \citep{takada_jain}; this also incorporates the number of galaxies in the
sample:
\begin{equation}
    \mathrm{Cov}(C^{ij}_\ell, C^{mn}_\ell) = \frac{1}{(2 \ell + 1)\Delta\ell f_\mathrm{sky}}(D^{im}_\ell + D^{jn}_\ell)(D^{in}_\ell + D^{jm}_\ell)
\end{equation}
where $D^{ij}_\ell = C^{ij}_\ell + N^{ij}_\ell$ and we assume an $f_\mathrm{sky}=0.25$.  The noise spectra are:
\begin{align}
N^{\mathrm{cluster},ij}_\ell = \delta_{ij} / n_i \\
N^{\mathrm{lensing},ij}_\ell = \delta_{ij} \sigma_e^2 / n_i
\end{align}
where $n_i$ is the number density of galaxies in bin $i$ (we asssume equally weighted galaxies, and a 
total number density over all bins of 20 galaxies per square arcminute) and $\sigma_e=0.26$.

\section{Mistakes}
The challenge organizers made a number of mistakes when building and running the challenge.
These do not invalidate the process in any way, but we describe them here in the hope of
being useful for future challenge designers.

\begin{itemize}
    \item Splitting classification from nominal bin choice
    \item Infrastructure for checking submissions
    \item Guidance for users regarding caching and batch processing
    \item Timing
    \item Training set size
    \item Advertisement
\end{itemize}

%=====================
% END OF THE MAIN TEXT
%=====================

\end{document}
