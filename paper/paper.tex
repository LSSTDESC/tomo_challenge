\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[twocolumn,twocolappendix]{aastex63}

% Add your own macros here:
\pdfoutput=1 %for arXiv submission
%\usepackage{amsmath,amssymb,amstext}
\usepackage{amsmath,amstext}
\usepackage[T1]{fontenc}
\usepackage{apjfonts}
\usepackage{ae,aecompl}
\usepackage[utf8]{inputenc}
\usepackage[figure,figure*]{hypcap}

\usepackage{url}
\urlstyle{same}

\usepackage{lineno}
\linenumbers
%\modulolinenumbers[2]

\newcommand{\placeholder}[1]{\textit{PLACEHOLDER: #1}}


% header settings
\shorttitle{Tomographic binning optimization}
\shortauthors{Author A et al.\ (LSST~DESC)}

% ======================================================================

\begin{document}
\title{The LSST DESC 3x2pt Tomography Optimization Challenge}
\input{authors}

\begin{abstract}
In this paper we present the results of the DESC 3x2pt tomography challenge, which aims to compare strategies for optimizing the photometric binning required for the main LSST 3x2pt analysis. This task is made particularly delicate in the context of a metacalibrated lensing survey, as only the photometry from the bands  included in the metacalibration process (riz and potentially g) can be used to define this tomographic binning. Using a set of realistic simulations including true galaxy redshifts and simulated photometry, the goal of the challenge is to propose a bin assignment strategy that maximizes the overall Figure of Merit of a 3x2pt analysis. This setting is idealized as it ignores spectroscopic completeness  issues for the training sample, but complex enough to address the main question of binning optimization. Based on the challenge dataset, we review and compare the different methods that have been proposed by participants. We find that even from this limited photometry information, various methods are able to yield binning schemes that achieve high FoM scores.
\end{abstract}

\keywords{methods: statistical -- dark energy  -- large-scale structure of the universe}

%\accepted{}
%\submitjournal{the Astrophysical Journal Supplement}


%\tableofcontents%-----------------------------
%===========================
% BEGINNING OF THE MAIN TEXT
%===========================

\section{Introduction}
Weak gravitational lensing (WL) has emerged over the last decade as a powerful
cosmological probe \citep{cfhtlens,des,kids,hsc}.  WL
uses measurements of coherent shear distortion to the observed shapes of galaxies
to track the evolution of large-scale gravitational fields.  It measures the integrated
gravitational potential along lines of sight to source galaxies, and can thence constrain
the laws of gravity, the expansion history of the Universe, and the history and growth
of cosmic structure.

WL has proven especially powerful in combination with galaxy clustering measurements,
which can measure the density of matter up to an unknown bias function.  The high signal
to noise of such measurements and the relative certainty of the redshift of these foreground
samples breaks degeneracies in the systematic errors that affect WL.

The \emph{3x2pt} method has become the standard method for performing this combination.
In this method, two-point correlations are computed among and between two samples, the shapes of 
background (source) galaxies and the locations of foreground galaxies, which trace foreground
dark matter haloes.  The three combinations (source-source, source-lens, and lens-lens) are
measured in either Fourier or configuration space, and can be predicted from a combination of 
perturbation theory and simulation results.  The method has been used in the Dark Energy Survey, DES
\citep{des-3x2pt}, and to combine Baryon Oscillation Spectroscopic Survey, BOSS, with the Kilo-Degree 
Survey, KiDS \citep{kids-3x2pt}.

Most lensing and 3x2pt analyses have been performed \emph{tomographically}, 
binning galaxies by redshift.
This approach captures almost all the available information in lensing data, since it measures
an integrated effect and so galaxies nearby in redshift probe very similar fields.  For photometric
foreground samples it is similarly near-lossless, since redshift estimates of such galaxies have
large uncertainties\footnote{Spectroscopic foreground samples may be more likely to see significant 
gains from moving beyond spectroscopic methods.}.  Binning galaxies by approximate redshift lets us 
model galaxy bias, intrinsic alignments, and other systematic errors en masse in a more tractable way.
While fully 3D methods have been proposed and prototyped \citep{kitching,more}, the tomographic 
approach remains the standard within the field.

Tomographic 3x2pt measurements will be a key science goal in the upcoming \emph{Stage IV} surveys,
including the Rubin Observatory \citep{rubin} and the Euclid and Roman space telescopes \citep{euclid,roman}.

In this paper we discuss and evaluate methods for assigning objects to tomographic bins, and 
specifically the impact
of using a limited set of color bands to make those assignments, motivated by requirements of using the
metacalibration method for galaxy shape measurement to limit biases.  Tomographic methods have been 
explored before in \citet{jain} and \citet{kitching2019}.  In this paper we describe the results of a 
challenge using simulated Rubin-like data with a range of methods submitted by different teams.

In \autoref{sec:motivation} we explain the need for methods that work on limited sets of bands in the context of upcoming lensing surveys.  \textbf{Complete structure...}

\section{Motivation}
\label{sec:motivation}

The methodology used to assign galaxies to bins faces special challenges when we use a particularly
effective approach for measuring galaxy shear, called \emph{metacalibration} (metacal),
which was introduced in \citet{sheldonhuff} and developed further in \citet{sheldon}.
In metacal, galaxy images are deconvolved from the point-spread function (PSF), sheared, and
reconvolved before measurement, allowing one to directly compute the response of a metric
to underlying shear and correct for it when computing two-point measurements.  This can
almost completely remove errors due to model and estimator (noise) bias from WL data, at least
when PSFs are well-measured and oversampled by the pixel grid.

Furthermore, metacal provides a solution to the pernicious and general problem of
\emph{selection biases} in WL.  Measurements of galaxy shear are known to have noise-induced errors
that are
highly covariant with those on other galaxy properties, including size and, most importantly, flux/
magnitude.  It follows that a cut (or re-weighting) on galaxy magnitudes, or on any quantity derived
from  them, will induce a shear bias since galaxies will preferentially fall either side of the cut
depending on their shear.  Within metacal, we can solve this problem by performing the cut on both
sheared variants of the catalog, and measuring the difference between the post-cut shear in the two
variants.  In DES the corrected biases were found up to around 3\%, far larger than
the requirements for the current generation of surveys \cite{des-y1-cat}.

In summary: metacal allows us to correct for significant selection biases, but only if all selections
are performed using only bands in which the PSF is measured well enough for deconvolution to be
performed.  For Rubin, this means using only the $r$, $i$, $z$, and perhaps $g$ bands.  In this work we
therefore study how well we can perform tomography using only this limited photometric information.  In
particular, this limitation prevent us from using the most obvious method for tomography,
and computnig a complete photometric probability density function (PDF) for each galaxy and assigning
a bin using the mean or peak of that PDF.

Simulation methods can also be used to correct for selection biases, provided that they match the
real data to high accuracy.  If we determine that limiting the bands we can use for tomography results
in a significant decrease in our overall signal-to-noise, outweighing the gain from the improved shape
calibration then this might suggest moving to rely more on such methods\footnote{One can account for
residual shear estimation uncertainty during parameter estimation by marginalizing over an unknown
factor.  Widening the prior on this factor decreases the overall constraining power of the analysis.}.


\begin{itemize}
    \item Goal of challenge: identify binning method(s) leading to best cosmology constraints 
\end{itemize}

\url{https://arxiv.org/abs/1901.06495}

\url{https://arxiv.org/abs/astro-ph/0609338}

\section{Challenge Design}

\subsection{Philosophy}
\begin{itemize}
    \item Simplest challenge (optimistic data)
    \item Openness?
    \item Science-relevant metrics
    \item Performance at scale
\end{itemize}

\subsection{Data}

\begin{itemize}
    \item CosmoDC2 - description
    \item CosmoDC2 - discussion
    \item Noise properties
    \item Pre-selection 
    \item Metacal as a limitation
\end{itemize}

\subsection{Metrics}
\label{sec:metrics}


\begin{itemize}
    \item S/N of spectra, CCL
    \item DETF Figure of merit, FireCrown(?)
    \item mutual information
\end{itemize}


\placeholder{want to check performance at multiple stages of a cosmology analysis}

\subsection{Infrastructure}
\begin{itemize}
    \item Pull request submissions
    \item Data at NERSC
\end{itemize}


\subsection{Control Methods}
\begin{itemize}
    \item single bin
    \item one galaxy per bin?
    \item sanity check: random selection should be same as single bin
\end{itemize}


\section{Methods}
\begin{itemize}
    \item Gradient Boosting \@ jecampagne
    \item Neural Network \@EiffL
    \item Random Forest
    \item \placeholder{cuts in photometry space without redshift estimation \@barber?}
\end{itemize}

\section{Results}
\begin{itemize}
    \item Big table of results
    \item fisher matrix ellipses overplotted for top N methods
    \item Plots of n(z)
\end{itemize}

\section{Discussion}
\begin{itemize}
    \item Overall effectiveness of method classes
    \item Individual winner
    \item Largest number of bins that proves workable
\end{itemize}

\subsection{Future directions}

\begin{itemize}
    \item spectroscopic incompleteness
\end{itemize}

\section{Conclusions}



%=====================
% END OF THE MAIN TEXT
%=====================

\end{document}
