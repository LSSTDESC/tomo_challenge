\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[twocolumn,twocolappendix]{aastex63}

% Add your own macros here:
\pdfoutput=1 %for arXiv submission
%\usepackage{amsmath,amssymb,amstext}
\usepackage{amsmath,amstext}
\usepackage[T1]{fontenc}
\usepackage{apjfonts}
\usepackage{ae,aecompl}
\usepackage[utf8]{inputenc}
\usepackage[figure,figure*]{hypcap}

\usepackage{url}
\urlstyle{same}

\usepackage{lineno}
\linenumbers
%\modulolinenumbers[2]

\newcommand{\placeholder}[1]{\textit{PLACEHOLDER: #1}}


% header settings
\shorttitle{Tomographic binning optimization}
\shortauthors{Author A et al.\ (LSST~DESC)}

% ======================================================================

\begin{document}
\title{The LSST DESC 3x2pt Tomography Optimization Challenge}
\input{authors}

\begin{abstract}
In this paper we present the results of the DESC 3x2pt tomography challenge, which aims to compare strategies for optimizing the photometric binning required for the main LSST 3x2pt analysis. This task is made particularly delicate in the context of a metacalibrated lensing survey, as only the photometry from the bands  included in the metacalibration process (riz and potentially g) can be used to define this tomographic binning. Using a set of realistic simulations including true galaxy redshifts and simulated photometry, the goal of the challenge is to propose a bin assignment strategy that maximizes the overall Figure of Merit of a 3x2pt analysis. This setting is idealized as it ignores spectroscopic completeness  issues for the training sample, but complex enough to address the main question of binning optimization. Based on the challenge dataset, we review and compare the different methods that have been proposed by participants. We find that even from this limited photometry information, various methods are able to yield binning schemes that achieve high FoM scores.
\end{abstract}

\keywords{methods: statistical -- dark energy  -- large-scale structure of the universe}

%\accepted{}
%\submitjournal{the Astrophysical Journal Supplement}


%\tableofcontents%-----------------------------
%===========================
% BEGINNING OF THE MAIN TEXT
%===========================

\section{Introduction}
Weak gravitational lensing (WL) has emerged over the last decade as a powerful
cosmological probe \citep{cfhtlens,des,kids,hsc}.  WL
uses measurements of coherent shear distortion to the observed shapes of galaxies
to track the evolution of large-scale gravitational fields.  It measures the integrated
gravitational potential along lines of sight to source galaxies, and can thence constrain
the laws of gravity, the expansion history of the Universe, and the history and growth
of cosmic structure.

WL has proven especially powerful in combination with galaxy clustering measurements,
which can measure the density of matter up to an unknown bias function.  The high signal
to noise of such measurements and the relative certainty of the redshift of these foreground
samples breaks degeneracies in the systematic errors that affect WL.

The \emph{3x2pt} method has become the standard method for performing this combination.
In this method, two-point correlations are computed among and between two samples, the shapes of 
background (source) galaxies and the locations of foreground galaxies, which trace foreground
dark matter haloes.  The three combinations (source-source, source-lens, and lens-lens) are
measured in either Fourier or configuration space, and can be predicted from a combination of 
perturbation theory and simulation results.  The method has been used in the Dark Energy Survey, DES
\citep{des-3x2pt}, and to combine Baryon Oscillation Spectroscopic Survey, BOSS, with the Kilo-Degree 
Survey, KiDS \citep{kids-3x2pt}.

Most lensing and 3x2pt analyses have been performed \emph{tomographically}, 
binning galaxies by redshift.
This approach captures almost all the available information in lensing data, since it measures
an integrated effect and so galaxies nearby in redshift probe very similar fields.  For photometric
foreground samples it is similarly near-lossless, since redshift estimates of such galaxies have
large uncertainties\footnote{Spectroscopic foreground samples may be more likely to see significant 
gains from moving beyond spectroscopic methods.}.  Binning galaxies by approximate redshift lets us 
model galaxy bias, intrinsic alignments, and other systematic errors en masse in a more tractable way.
While fully 3D methods have been proposed and prototyped \citep{kitching,more}, the tomographic 
approach remains the standard within the field.

Tomographic 3x2pt measurements will be a key science goal in the upcoming \emph{Stage IV} surveys,
including the Rubin Observatory \citep{rubin} and the Euclid and Roman space telescopes \citep{euclid,roman}.

In this paper we discuss and evaluate methods for assigning objects to tomographic bins, and 
specifically the impact
of using a limited set of color bands to make those assignments, motivated by requirements of using the
metacalibration method for galaxy shape measurement to limit biases.  Tomographic methods have been 
explored before in \citet{jain} and \citet{kitching2019}.  In this paper we describe the results of a 
challenge using simulated Rubin-like data with a range of methods submitted by different teams.  We explore how many tomographic bins we can effectively generate using only a subset of bins, and compare methods submitted to the challenge.

In \autoref{sec:motivation} we explain the need for methods that work on limited sets of bands in the context of upcoming lensing surveys.  \textbf{Complete structure...}

\section{Motivation}
\label{sec:motivation}

The methodology used to assign galaxies to bins faces special challenges when we use a particularly
effective approach for measuring galaxy shear, called \emph{metacalibration} (metacal),
which was introduced in \citet{sheldonhuff} and developed further in \citet{sheldon}.
In metacal, galaxy images are deconvolved from the point-spread function (PSF), sheared, and
reconvolved before measurement, allowing one to directly compute the response of a metric
to underlying shear and correct for it when computing two-point measurements.  This can
almost completely remove errors due to model and estimator (noise) bias from WL data, at least
when PSFs are well-measured and oversampled by the pixel grid.

Furthermore, metacal provides a solution to the pernicious and general problem of
\emph{selection biases} in WL.  Measurements of galaxy shear are known to have noise-induced errors
that are
highly covariant with those on other galaxy properties, including size and, most importantly, flux/
magnitude.  It follows that a cut (or re-weighting) on galaxy magnitudes, or on any quantity derived
from  them, will induce a shear bias since galaxies will preferentially fall either side of the cut
depending on their shear.  Within metacal, we can solve this problem by performing the cut on both
sheared variants of the catalog, and measuring the difference between the post-cut shear in the two
variants.  In DES the corrected biases were found up to around 3\%, far larger than
the requirements for the current generation of surveys \cite{des-y1-cat}.

In summary: metacal allows us to correct for significant selection biases, but only if all selections
are performed using only bands in which the PSF is measured well enough for deconvolution to be
performed.  For Rubin, this means using only the $r$, $i$, $z$, and perhaps $g$ bands.  In this work we
therefore study how well we can perform tomography using only this limited photometric information.  In
particular, this limitation prevent us from using the most obvious method for tomography,
and computnig a complete photometric probability density function (PDF) for each galaxy and assigning
a bin using the mean or peak of that PDF.

Simulation methods can also be used to correct for selection biases, provided that they match the
real data to high accuracy.  If we determine that limiting the bands we can use for tomography results
in a significant decrease in our overall signal-to-noise, outweighing the gain from the improved shape
calibration then this might suggest moving to rely more on such methods\footnote{One can account for
residual shear estimation uncertainty during parameter estimation by marginalizing over an unknown
factor.  Widening the prior on this factor decreases the overall constraining power of the analysis.}.


\begin{itemize}
    \item Goal of challenge: identify binning method(s) leading to best cosmology constraints 
\end{itemize}


\section{Challenge Design}

The Dark Energy Science Collaboration (DESC) Tomographic Challenge was launched in May 2019 and 
advertised among the LSST cosmology community, though it was open to outside entrants.

In the challenge, participants assigned simulated galaxies to tomographic bins,
using only their (g)riz bands and associated errors.  Their goal was to maximize the cosmological
information in the final split data set; this is generally acheived by separating different bins by 
redshift as much as possible.  Particpants were free to optimize or simply select nominal (target) 
redshift bin edges from their training sample, but in either case had to classify galaxies into the 
different bins.

\subsection{Philosophy}

Since this was a preliminary challenge designed to explore whether it is possible \emph{in theory}
to use only (g)riz for tomography, we chose to simplify several aspects of the data.  In realistic 
situations we will have access to limited sized training sets for WL photometric redshifts, since 
spectra are time-consuming to obtain.  These training sets will also be highly incomplete compared
to full catalogs, especially at faint magnitudes where spectroscopic coverage is sparse.  In this
challenge we avoided both these issues -- the training samples we provided (see below) were comparable
in size to the testing sample, and drawn from the same population.  Given this, the challenge
represents a best-case scenario -- if no method succeeded on this easier case then more realistic
cases would probably be impossible.

Despite these simplifications, the other aspects of the challenge and process were designed to be
as directly relevant as possible to the particular WL science case we focus on.  The data set
was chosen to mimic the population, noise, and cuts we will use in real data (see \autoref{sec:data}, 
and the metrics were designed to be as close to the science goals as possible, rather than a lower
level representation (see \autoref{sec:metrics}). The data set was also large enough to pose a 
reasonably realistic test of methods at the large data volume required for upcoming surveys, where
$10^9$ -- $10^{10}$ galaxies will be measured.

The challenge was open to any entrants, not just those already involved in DESC, but it was
advertised primarily through Rubin, and so is perhaps best described as semi-public.
Participants tested and ran their methods locally, but submitted code to be run as part of a central
combined analysis.  No prize was offered apart from recognition.



\begin{itemize}
    \item Performance at scale
\end{itemize}

\subsection{Data}
\label{sec:data}


\begin{itemize}
    \item CosmoDC2 - description
    \item CosmoDC2 - discussion
    \item Noise properties
    \item Pre-selection 
    \item Metacal as a limitation
\end{itemize}

\subsection{Metrics}
\label{sec:metrics}



In this challenge we use only a source (background) population, since the metacal requirements
do not apply to the foreground lens sample. We do, however, calculate 

\begin{itemize}
    \item S/N of spectra, CCL
    \item DETF Figure of merit, FireCrown(?)
    \item mutual information
\end{itemize}


\placeholder{want to check performance at multiple stages of a cosmology analysis}

\subsection{Infrastructure}
\begin{itemize}
    \item Pull request submissions
    \item Data at NERSC
\end{itemize}


\subsection{Control Methods}
\begin{itemize}
    \item single bin
    \item one galaxy per bin?
    \item sanity check: random selection should be same as single bin
\end{itemize}


\section{Methods}
\begin{itemize}
    \item Gradient Boosting \@ jecampagne
    \item Neural Network \@EiffL
    \item Random Forest
    \item \placeholder{cuts in photometry space without redshift estimation \@barber?}
\end{itemize}

\section{Results}
\begin{itemize}
    \item Big table of results
    \item fisher matrix ellipses overplotted for top N methods
    \item Plots of n(z)
\end{itemize}

\section{Discussion}
\begin{itemize}
    \item Overall effectiveness of method classes
    \item Individual winner
    \item Largest number of bins that proves workable
\end{itemize}

\subsection{Future directions}

\begin{itemize}
    \item spectroscopic incompleteness
\end{itemize}

\section{Conclusions}

\appendix{Mistakes}
The challenge organizers made a number of mistakes when building and running the challenge.
These do not invalidate the process in any way, but we describe them here in the hope of
being useful for future challenge designers.

\begin{itemize}
    \item Infrastructure for checking submissions
    \item Guidance for users regarding caching and batch processing
    \item Timing
    \item Training set size
    \item Advertisement
\end{itemize}

%=====================
% END OF THE MAIN TEXT
%=====================

\end{document}
