\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[twocolumn,twocolappendix]{aastex63}

% Add your own macros here:
\pdfoutput=1 %for arXiv submission
%\usepackage{amsmath,amssymb,amstext}
\usepackage{amsmath,amstext}
\usepackage[T1]{fontenc}
\usepackage{apjfonts}
\usepackage{ae,aecompl}
\usepackage[utf8]{inputenc}
\usepackage[figure,figure*]{hypcap}
\usepackage{natbib}
\usepackage{url}
\usepackage{listings}

\urlstyle{same}

\usepackage{lineno}
\linenumbers
%\modulolinenumbers[2]

\newcommand{\placeholder}[1]{\textit{PLACEHOLDER: #1}}


% header settings
\shorttitle{Tomographic binning optimization}
\shortauthors{Author A et al.\ (LSST~DESC)}

% ======================================================================

\begin{document}
\title{The LSST DESC 3x2pt Tomography Optimization Challenge}
\input{authors}

\begin{abstract}
In this paper we present the results of the DESC 3x2pt tomography challenge, which aims to compare strategies for optimizing the photometric binning required for the main LSST 3x2pt analysis. This task is made particularly delicate in the context of a metacalibrated lensing survey, as only the photometry from the bands  included in the metacalibration process (riz and potentially g) can be used to define this tomographic binning. Using a set of realistic simulations including true galaxy redshifts and simulated photometry, the goal of the challenge is to propose a bin assignment strategy that maximizes the overall Figure of Merit of a 3x2pt analysis. This setting is idealized as it ignores spectroscopic completeness  issues for the training sample, but complex enough to address the main question of binning optimization. Based on the challenge dataset, we review and compare the different methods that have been proposed by participants. We find that even from this limited photometry information, various methods are able to yield binning schemes that achieve high FoM scores.
\end{abstract}

\keywords{methods: statistical -- dark energy  -- large-scale structure of the universe}

%\accepted{}
%\submitjournal{the Astrophysical Journal Supplement}


%\tableofcontents%-----------------------------
%===========================
% BEGINNING OF THE MAIN TEXT
%===========================

\section{Introduction}
Weak gravitational lensing (WL) has emerged over the last decade as a powerful
cosmological probe \citep{cfhtlens,des,kids,hsc}.  WL
uses measurements of coherent shear distortion to the observed shapes of galaxies
to track the evolution of large-scale gravitational fields.  It measures the integrated
gravitational potential along lines of sight to source galaxies, and can thence constrain
the laws of gravity, the expansion history of the Universe, and the history and growth
of cosmic structure.

WL has proven especially powerful in combination with galaxy clustering measurements,
which can measure the density of matter up to an unknown bias function.  The high signal
to noise of such measurements and the relative certainty of the redshift of these foreground
samples breaks degeneracies in the systematic errors that affect WL.

The \emph{3x2pt} method has become the standard method for performing this combination.
In this method, two-point correlations are computed among and between two samples, the shapes of 
background (source) galaxies and the locations of foreground galaxies, which trace foreground
dark matter haloes.  The three combinations (source-source, source-lens, and lens-lens) are
measured in either Fourier or configuration space, and can be predicted from a combination of 
perturbation theory and simulation results.  The method has been used in the Dark Energy Survey, DES
\citep{des-3x2pt}, and to combine Baryon Oscillation Spectroscopic Survey, BOSS, with the Kilo-Degree 
Survey, KiDS \citep{kids-3x2pt}.

Most lensing and 3x2pt analyses have been performed \emph{tomographically}, 
binning galaxies by redshift.
This approach captures almost all the available information in lensing data, since it measures
an integrated effect and so galaxies nearby in redshift probe very similar fields.  For photometric
foreground samples it is similarly near-lossless, since redshift estimates of such galaxies have
large uncertainties\footnote{Spectroscopic foreground samples may be more likely to see significant 
gains from moving beyond spectroscopic methods.}.  Binning galaxies by approximate redshift lets us 
model galaxy bias, intrinsic alignments, and other systematic errors en masse in a more tractable way.
While fully 3D methods have been proposed and prototyped \citep{kitching,more}, the tomographic 
approach remains the standard within the field.

Tomographic 3x2pt measurements will be a key science goal in the upcoming \emph{Stage IV} surveys,
including the Rubin Observatory \citep{rubin} and the Euclid and Roman space telescopes \citep{euclid,roman}.

In this paper we discuss and evaluate methods for assigning objects to tomographic bins, and 
specifically the impact
of using a limited set of color bands to make those assignments, motivated by requirements of using the
metacalibration method for galaxy shape measurement to limit biases.  Tomographic methods have been 
explored before in \citet{jain} and \citet{kitching2019}.  In this paper we describe the results of a 
challenge using simulated Rubin-like data with a range of methods submitted by different teams.  We explore how many tomographic bins we can effectively generate using only a subset of bins, and compare methods submitted to the challenge.

In \autoref{sec:motivation} we explain the need for methods that work on limited sets of bands in the context of upcoming lensing surveys.  \textbf{Complete structure...}

\section{Motivation}
\label{sec:motivation}

The methodology used to assign galaxies to bins faces special challenges when we use a particularly
effective approach for measuring galaxy shear, called \emph{metacalibration} (metacal),
which was introduced in \citet{sheldonhuff} and developed further in \citet{sheldon}.
In metacal, galaxy images are deconvolved from the point-spread function (PSF), sheared, and
reconvolved before measurement, allowing one to directly compute the response of a metric
to underlying shear and correct for it when computing two-point measurements.  This can
almost completely remove errors due to model and estimator (noise) bias from WL data, at least
when PSFs are well-measured and oversampled by the pixel grid.

Furthermore, metacal provides a solution to the pernicious and general problem of
\emph{selection biases} in WL.  Measurements of galaxy shear are known to have noise-induced errors
that are
highly covariant with those on other galaxy properties, including size and, most importantly, flux/
magnitude.  It follows that a cut (or re-weighting) on galaxy magnitudes, or on any quantity derived
from  them, will induce a shear bias since galaxies will preferentially fall either side of the cut
depending on their shear.  Within metacal, we can solve this problem by performing the cut on both
sheared variants of the catalog, and measuring the difference between the post-cut shear in the two
variants.  In DES the corrected biases were found up to around 3\%, far larger than
the requirements for the current generation of surveys \cite{des-y1-cat}.

In summary: metacal allows us to correct for significant selection biases, but only if all selections
are performed using only bands in which the PSF is measured well enough for deconvolution to be
performed.  For Rubin, this means using only the $r$, $i$, $z$, and perhaps $g$ bands.  In this work we
therefore study how well we can perform tomography using only this limited photometric information.  In
particular, this limitation prevent us from using the most obvious method for tomography,
and computnig a complete photometric probability density function (PDF) for each galaxy and assigning
a bin using the mean or peak of that PDF.

Simulation methods can also be used to correct for selection biases, provided that they match the
real data to high accuracy.  If we determine that limiting the bands we can use for tomography results
in a significant decrease in our overall signal-to-noise, outweighing the gain from the improved shape
calibration then this might suggest moving to rely more on such methods\footnote{One can account for
residual shear estimation uncertainty during parameter estimation by marginalizing over an unknown
factor.  Widening the prior on this factor decreases the overall constraining power of the analysis.}.


\section{Challenge Design}

The Dark Energy Science Collaboration (DESC) Tomographic Challenge was launched in May 2019 and 
advertised among the LSST cosmology community, though it was open to outside entrants.

In the challenge, participants assigned simulated galaxies to tomographic bins,
using only their (g)riz bands and associated errors.  Their goal was to maximize the cosmological
information in the final split data set; this is generally acheived by separating different bins by 
redshift as much as possible.  Particpants were free to optimize or simply select nominal (target) 
redshift bin edges from their training sample, but in either case had to classify galaxies into the 
different bins.

\subsection{Philosophy}

Since this was a preliminary challenge designed to explore whether it is possible \emph{in theory}
to use only (g)riz for tomography, we chose to simplify several aspects of the data.  In realistic 
situations we will have access to limited sized training sets for WL photometric redshifts, since 
spectra are time-consuming to obtain.  These training sets will also be highly incomplete compared
to full catalogs, especially at faint magnitudes where spectroscopic coverage is sparse.  In this
challenge we avoided both these issues -- the training samples we provided (see below) were comparable
in size to the testing sample, and drawn from the same population.  Given this, the challenge
represents a best-case scenario -- if no method succeeded on this easier case then more realistic
cases would probably be impossible.

Despite these simplifications, the other aspects of the challenge and process were designed to be
as directly relevant as possible to the particular WL science case we focus on.  The data set
was chosen to mimic the population, noise, and cuts we will use in real data (see \autoref{sec:data}, 
and the metrics were designed to be as close to the science goals as possible, rather than a lower
level representation (see \autoref{sec:metrics}). The data set was also large enough to pose a 
reasonably realistic test of methods at the large data volume required for upcoming surveys, where
$10^9$ -- $10^{10}$ galaxies will be measured.

The challenge was open to any entrants, not just those already involved in DESC, but it was
advertised primarily through Rubin, and so is perhaps best described as semi-public.
Participants tested and ran their methods locally, but submitted code to be run as part of a central
combined analysis.  No prize was offered beyond recognition.


\subsection{Data}
\label{sec:data}


\begin{itemize}
    \item CosmoDC2 - description
    \item CosmoDC2 - discussion
    \item Noise properties
    \item Pre-selection 
    \item Metacal as a limitation
\end{itemize}

\subsection{Metrics}
\label{sec:metrics}

In this challenge we use only a lensing source (background) population, since the metacal requirements
do not apply to the foreground lens sample. We do, however, calculate 3x2pt metrics using the
same source and lens populations, both because this scenario is one that will be used in some analyses,
and because clustering-like statistics of lensing samples are important for studies of intrinsic
alignments. We use two different sets of metrics, one based on the over signal to noise of the spectra 
derived from the samples, and one based on a figure-of-merit for the constraints that would be obtained 
using them.  \textbf{Discuss how correlated these turn out to be here}.  

For each bin $i$ generated by a challenge entry, we make a histogram of the true redshifts 
of each galaxy assigned to the bin.  This is then used as our number density $n_i(z)$ in the metrics 
below. Both metrics reward bins that can
cleanly separate galaxies by redshift, since this reduces the covariance between the samples. No
method can perfectly separate them, so good methods reduce the tails of $n_i(z)$ that overlap with 
neighbouring bins.

We developed two implementations of each of these metrics, one using the Core Cosmology Library 
\citep{ccl} and one the JAX Cosmology Library \citep{jax-cosmo}, which provides differentiable theory 
predictions using the JAX library \citep{jax}.  In production we use the latter since it provided a
more stable calculation of metric 2.

Metric 1 would be approximately measurable on real data without performing a full cosmology analysis,
whereas metric 2 would be the final output of such an analysis.  We therefore simulate performance
tests at multiple stages in the wider process.


\subsubsection{Metric 1: Sigal-to-noise}
Our first metric is the total signal-to-noise ratio of the power spectrum derived from the assigned
$n_i(z)$ values:
\begin{equation}
    S = \mu^{T} C^{-1} \mu
\end{equation}
where $\mu$ is a stack of the theoretical predictions for the $C_\ell$ spectra for each tomographic 
bin, in turn, and $C$ a Gaussian estimaate of the covariance between them, as described in
\citet{wl-review} and summarized in \autoref{app:theory}.   We compute this metric for lensing alone, clustering alone, and the full 3x2pt combination.

\subsubsection{Metric 2: Figure-of-merit}

Metric 2 uses a figure of merit based on that presented by the Dark Energy Task Force \citep{detf}.  We 
use the inverse area of a Fisher Matrix ellipse, which approximates the size in two axes 
of the posterior PDF one would obtain on analysing the spectra:
\begin{align}
    F &= \left( \frac{\partial \mu}{\partial \theta} \right)^T C^{-1} \left( \frac{\partial \mu}{\partial \theta} \right), \\
    S &= \frac{1}{2 \pi \sqrt{\det{([F^{-1}]_{p_1, p_2})}}}
\end{align}
where $[F^{-1}]_{p_1, p_2}$ extracts the 2x2 matrix for two parameters.  We use both the original
DETF variant in which $p_1 = w_0, p_2 = w_a$, representing the constraining power on dark energy 
evolution, and a version in which $p_1 = \Omega_c$ and $p_2 = \sigma_8$, representing constraining
power on overall cosmic structure amplitude and its evolution.




\subsection{Infrastructure}
\begin{itemize}
    \item Pull request submissions
    \item Data at NERSC
\end{itemize}


\subsection{Control Methods}

Two control methods were used to test the challenge machinery and metrics.
The first method randomly assigned galaxies to each tomographic bin, resulting
in bins which each had the same $n(z)$, on average.  Using this method, we expect
that increasing the number of bins should not increase any metric, since bins are
maximally correlated.  We find this to be true for all our metrics.

Second, we use a method which uses only the $i$-band to select tomographic bins,
splitting galaxies in equal-sized bins in this magnitude. We expect only a small
increase in metrics with the number of bins, and also that the addition of the $g$-band
should make no difference to metrics.  Again, we find this to be true.

One additional method was submitted to the challenge with the caveat that it could
never be applied to real data and should therefore considered a form of control method.
This method was {\sc Utopia}, which found, for each object in the testing data, the
nearest neighbour object in the training sample.  The high density, completeness, and
representativeness of our training set made this possible; realistics spectroscopic training
sets are too sparse.  The method performed well, as expected, 
but was not the highest scoring overall.


\section{Methods}

\subsection{ {\sc RandomForest} }
This method was submitted by the challenge organizers and used a random
forest algorithm to assign galaxies.

Random forests \cite{breiman2001} train a collection of individual \emph{decision trees},
each of which consists of a set of bifurcating comparisons in which different features in
the data (in this case, magnitudes and errors) are compared to criterion values to choose
which branch to follow.  Each ``leaf'' (end comparison) selects a classification bin for
an input galaxy, and the tree is trained to choose comparison features and comparison values
which maximally split the discrimination at each step, and in final leaf purity.

Since decision trees tend to over-fit data, random forests generate a suite of decision
trees, each randomly choosing a subset of features on which to train at each bifurcation.
An average over the trained trees is taken as the overall classification.

In this implementation we chose nominal bin edges by splittng ensuring equal numbers of training set 
objects were in each bin, and then trained the forest to map magnitudes, magnitude errors, and
galaxy sizes to bin indices.  We use the {\sc scikit-learn} implementation of the random forest
\citep{scikit-learn}.


% Please describe your methods with some very general / high level background theory
% and a focus on the spsecific implementation.  Add any citations you like with bibtex in
% paper.bib
% Feel free to rename your method, but please leave a comment explaining so that I can do the
% same in the tables.

\subsection{ {\sc Autokeras LSTM} }


\subsection{ {\sc CNN} }


\subsection{ {\sc ComplexSOM} }


\subsection{ {\sc ENSEMBLE} }


\subsection{ {\sc GPzBinning} }


\subsection{ {\sc JaxCNN} }


\subsection{ {\sc JaxResNet} }


\subsection{ {\sc LGBM} }


\subsection{ {\sc LSTM} }


\subsection{ {\sc NeuralNetwork 1} }


\subsection{ {\sc NeuralNetwork 2} }


\subsection{ {\sc PCACluster} }


\subsection{ {\sc SimpleSOM} }


\subsection{ {\sc TCN} }


\subsection{ {\sc UTOPIA} }


\subsection{ {\sc ZotBin} }


\subsection{ {\sc ZotNet} }


\subsection{ {\sc funbins} }


\subsection{ {\sc mlpqna} }


\subsection{ {\sc Stacked Generalization} }

\emph{Stacked generalization} consists of stacking the output of several individual estimators and 
using a classifier to compute the final prediction. Stacking methods use the strength of each 
individual estimator by using their outputs as input of a final estimator.  This entry stacked
a \emph{Gradient Tree Boosting} (GTB) classifier and then used a Logistic Regression as the final step.

GTB is a machine learning method which combines decision trees (like the Random Forest, RF)
by performing a weighted mean average of individual trees \citep{Friedman:2002we,RefWorks:1634}. 
The prediction $F(X)$ of the ensemble of trees for a new sample $X$ is given by
\begin{equation}
F(X) = \sum_k w_k F_k(X)
\end{equation}
where $F_k(X)$ is the individual tree answer and $w_k$ a weight per tree. Unlike the Random Forest, the 
tree depth is rather small and the training  of the $k$-th tree is based on the errors of the
$(k-1)$-th tree.  Like the RF, a randomly chosen fraction of features ($\approx 50\%$) is 
omitted at each split, and this step is changed at each tree (leading to a Stochastic Gradient Descent 
algorithm).  GTB learning is essentially non-parallelizable and so cannot use a map-reduce paradigm, 
unlike RF.It suffers from the opposite problem to RF's overfitting, and is subject
to high bias, so one generally combines many weak individual learners.

The two methods have their own systematics; they notably differ in the feature importances. Stacking 
the two is therefore especially effective.  The submitted method used {\sc SKLearn}, and used 50 models 
of each type combined using its {\sc StackingClassifier}.

An iterative precedure was used to progressively change the target bin edges to optimize one of the metrics (FOMs or SNRs) between the trainings of the classifiers. In general, we recover different choices for each statistic, but the differences are relatively marginal. This procedure was done by hand for this challenge, but could be automated and applied to any kind of classifier.


% \lstset{language=Python}
% \lstset{frame=lines}
% \lstset{caption={The code}}
% \lstset{label={lst:code_direct}}
% \lstset{basicstyle=\footnotesize}
% \begin{lstlisting}
% estimators = [ 
% ('gd', make_pipeline(StandardScaler(),
%     GradientBoostingClassifier(n_estimators=n_estimators, verbose=1))),
 
% ('rf', make_pipeline(StandardScaler(),
%     RandomForestClassifier(n_estimators=n_estimators,verbose=1)))  ]
        
% classifier = StackingClassifier(estimators=estimators,
%                                final_estimator=LogisticRegression(max_iter=5000))
% \end{lstlisting}




\section{Results}

\begin{table*}[]
\begin{tabular}{l|llll|llll|}
                & \multicolumn{4}{c|}{\textbf{riz}}      & \multicolumn{4}{c|}{\textbf{griz}}                                \\ \hline
\textbf{Method} & \textbf{3-bin} & \textbf{5-bin} & \textbf{7-bin} & \textbf{9-bin} & \textbf{3-bin} & \textbf{5-bin} & \textbf{7-bin} & \textbf{9-bin} \\ \hline
{\sc Autokeras\_LSTM } & 31.1 & 74.9    & 103.4    & nan    & 44.1             & 67.5             & 122.2             & 98.4\\
{\sc CNN } & 27.1 & 50.8    & 76.7    & 103.3    & 30.5             & 57.7             & 95.4             & 122.4\\
{\sc ComplexSOM } & 38.0 & 52.1    & 94.4    & 101.6    & 34.9             & 45.0             & 91.6             & 100.3\\
{\sc ENSEMBLE1 } & 65.6 & 65.6    & 94.8    & 0.0    & 0.0             & 0.0             & 0.0             & 0.0\\
{\sc funbins } & 36.8 & 82.5    & 122.1    & 141.8    & 42.1             & 100.4             & 142.5             & 167.2\\
{\sc GPzBinning } & 26.2 & 49.7    & 80.8    & 111.7    & 27.8             & 55.3             & 87.2             & 126.9\\
{\sc IBandOnly } & 38.0 & 50.0    & 54.4    & 57.3    & 38.0             & 50.0             & 54.4             & 57.3\\
{\sc JaxCNN } & 59.9 & 101.9    & 105.2    & nan    & 79.7             & 125.2             & 150.0             & nan\\
{\sc JaxResNet } & 73.7 & 111.1    & 131.6    & nan    & 82.2             & 126.0             & nan             & 161.5\\
{\sc LGBM } & 27.0 & 50.8    & 78.8    & 108.2    & 30.3             & 57.4             & 92.9             & 125.2\\
{\sc LSTM } & 26.7 & 51.1    & 81.4    & 103.6    & 30.2             & 57.1             & 95.9             & 126.9\\
{\sc mlpqna } & 39.3 & 64.7    & 93.2    & 121.8    & 42.8             & 72.2             & 109.0             & 133.7\\
{\sc myCombinedClassifiers } & 35.3 & 60.5    & 83.5    & 120.4    & 39.4             & 63.9             & 93.1             & 148.9\\
{\sc NeuralNetwork\_1 } & 0.0 & 0.0    & 0.0    & 0.0    & 0.0             & 0.0             & 0.0             & 0.0\\
{\sc NeuralNetwork\_2 } & 0.0 & 0.0    & 0.0    & 0.0    & 0.0             & 0.0             & 0.0             & 0.0\\
{\sc PCACluster } & 29.5 & 68.1    & 75.8    & 0.0    & 50.1             & 72.7             & 90.8             & 0.0\\
{\sc Random } & 1.2 & 1.2    & 1.2    & 1.2    & 1.2             & 1.2             & 1.2             & 1.2\\
{\sc RandomForest } & 39.5 & 65.2    & 93.2    & 118.5    & 43.0             & 72.8             & 110.1             & 136.5\\
{\sc SimpleSOM } & 39.5 & 59.9    & 77.9    & 98.5    & 42.2             & 73.2             & 108.0             & 134.8\\
{\sc TCN } & 40.0 & 65.3    & 90.9    & 110.5    & 44.1             & 72.8             & 108.0             & 134.0\\
{\sc UTOPIA } & 39.0 & 63.7    & 90.8    & 112.2    & 43.1             & 74.0             & 112.8             & 139.3\\
{\sc ZotBin } & 64.8 & 106.4    & 121.8    & 135.3    & 77.6             & 120.0             & 141.8             & 154.7\\
{\sc ZotNet } & 73.6 & 111.2    & 131.8    & 145.9    & 83.7             & 128.5             & 150.1             & 167.2\\
\end{tabular}
\caption{INCOMPLETE, AND AUTO-GENERATED SO NO POINT EDITING! Values of the 3x2pt DETF figure-of-merit achieved by entrant methods on the 
CosmoDC2 part of the challenge.}
\end{table*}

\begin{itemize}
    \item Big table of results
    \item fisher matrix ellipses overplotted for top N methods
    \item Plots of n(z)
\end{itemize}

\section{Discussion}
\begin{itemize}
    \item Overall effectiveness of method classes
    \item Individual winner
\end{itemize}



\subsection{Individual winners}

RUN WINNERS WITH UGRIZY TO GET AN IDEA OF THE LOSS FROM DROPPING U AND Y.

\subsection{Large bin numbers}

It is clear from values above that fears about restricted photometry catastrophically limiting
the information needed for tomography are unfounded.  In both the CosmoDC2 and Buzzard analyses,
even the $riz$ bands alone are sufficient to split objects into nine reasonably separated
tomographic bins.
Once bins become this narrow the primary analysis concern will become the calibration of overall
bin $n(z)$ values, rather than initial tomography, so we consider this to be reassuring.

The inclusion of the $g$ band typically increases metrics by around 10--20\%, including for the
highest scoring methods.  This value should be weighed against the challenge of high-accuracy
PSF measurement for this band, but until the end of the survey this is unlikely to be
the limiting factor in overall precision.

\subsection{Future directions}

\begin{itemize}
    \item spectroscopic incompleteness
\end{itemize}

\section{Conclusions}

\bibliography{paper}


\appendix

\section{Theory}\label{app:theory}
In our metrics, the power spectrum between two tomographic bins $i$ and $j$ is computed using either the Core Cosmology Library \citep{ccl} or the JAX Cosmology Library \citep{jax-cosmo} as:
\begin{equation}
    C^{ij}_\ell = \int_0^{\infty} \frac{q_i(\chi) q_j(\chi)}{\chi^2} P\left(\frac{\ell +\frac{1}{2}}{\chi}, \chi \right) \mathrm{d}\chi
\end{equation}
where $\chi = \chi(z)$ is the comoving distance and $P$ the non-linear matter power spectrum at
a fiducial cosmology.  The kernel functions $q_i(\chi)$ are different for the lensing and clustering samples:
\begin{align}
    q^{\mathrm{cluster}}_i(\chi) &= n_i(\chi)\\
    q^{\mathrm{lensing}}_i(\chi) &= \frac{3}{2}\Omega_m \left(\frac{\mathrm{H}_0}{c}\right)^2 \frac{\chi}{a(\chi)} \int_\chi^{\infty} \frac{\chi' - \chi}{\chi'} n_i(\chi')\,\,\mathrm{d}\chi'
\end{align}
where $n_i(\chi) = n_i(z) \frac{\mathrm{d}z}{\mathrm{d}\chi}$.

We evaluate these at 100 $\ell$ values from $\ell_\mathrm{min}=100$ to $\ell_\mathrm{max}=2000$.

We use a Gaussian estimate for the covariance \citep{takada_jain}; this also incorporates the number of galaxies in the
sample:
\begin{equation}
    \mathrm{Cov}(C^{ij}_\ell, C^{mn}_\ell) = \frac{1}{(2 \ell + 1)\Delta\ell f_\mathrm{sky}}(D^{im}_\ell + D^{jn}_\ell)(D^{in}_\ell + D^{jm}_\ell)
\end{equation}
where $D^{ij}_\ell = C^{ij}_\ell + N^{ij}_\ell$ and we assume an $f_\mathrm{sky}=0.25$.  The noise spectra are:
\begin{align}
N^{\mathrm{cluster},ij}_\ell = \delta_{ij} / n_i \\
N^{\mathrm{lensing},ij}_\ell = \delta_{ij} \sigma_e^2 / n_i
\end{align}
where $n_i$ is the number density of galaxies in bin $i$ (we asssume equally weighted galaxies, and a 
total number density over all bins of 20 galaxies per square arcminute) and $\sigma_e=0.26$.

\section{Mistakes}
The challenge organizers made a number of mistakes when building and running the challenge.
These do not invalidate the process in any way, but we describe them here in the hope of
being useful for future challenge designers.

\begin{itemize}
    \item Splitting classification from nominal bin choice
    \item Infrastructure for checking submissions
    \item Guidance for users regarding caching and batch processing
    \item Timing
    \item Training set size
    \item Advertisement
\end{itemize}

%=====================
% END OF THE MAIN TEXT
%=====================

\end{document}
